{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../') #act as if we are one directory higher so imports work \n",
    "import torch\n",
    "from latent_to_timestep_model import LTT_model\n",
    "from dataset import load_data_from_dir\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import torch.nn.functional as F\n",
    "from dataset import LTTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0\"\n",
    "steps = 5\n",
    "latents, targets, conditions, unconditions, optimal_params = load_data_from_dir(data_folder=path, limit=50, use_optimal_params=True, steps=steps)\n",
    "#optimal_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LTTDataset(dir = \"/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0/validation\",\n",
    "                     size = 1000,\n",
    "                     train_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_outputs = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    _, second_output, _ = dataset[i]\n",
    "    second_outputs.append(second_output)\n",
    "len(second_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LD3 Best timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n3_params:\n",
      "tensor([0.1427, 0.2178, 0.1468, 0.4927], device='cuda:0')\n",
      "n5_params:\n",
      "tensor([0.1140, 0.1652, 0.1298, 0.1056, 0.1084, 0.3770], device='cuda:0')\n",
      "n7_params:\n",
      "tensor([0.1300, 0.1432, 0.1124, 0.0760, 0.1054, 0.0500, 0.0974, 0.2857],\n",
      "       device='cuda:0')\n",
      "n10_params:\n",
      "tensor([0.1337, 0.0979, 0.1225, 0.0529, 0.0616, 0.0705, 0.1014, 0.0505, 0.0802,\n",
      "        0.0948, 0.1340], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n3_params = torch.tensor([0.6048, 1.0274, 0.6334, 1.8439], device='cuda:0')\n",
    "n5_params = torch.tensor([0.8088, 1.1801, 0.9390, 0.7322, 0.7591, 2.0050], device='cuda:0')\n",
    "n7_params = torch.tensor([1.1434, 1.2401, 0.9985, 0.6071, 0.9339, 0.1873, 0.8551, 1.9311], device='cuda:0')\n",
    "n10_params = torch.tensor([1.6245, 1.3128, 1.5374, 0.6975, 0.8498, 0.9843, 1.3483, 0.6511, 1.1129, 1.2806, 1.6264], device='cuda:0')\n",
    "\n",
    "\n",
    "n3_params = F.softmax(n3_params, dim=0)\n",
    "n5_params = F.softmax(n5_params, dim=0)\n",
    "n7_params = F.softmax(n7_params, dim=0)\n",
    "n10_params = F.softmax(n10_params, dim=0)\n",
    "\n",
    "print(f\"n3_params:\\n{n3_params}\")\n",
    "print(f\"n5_params:\\n{n5_params}\")\n",
    "print(f\"n7_params:\\n{n7_params}\")\n",
    "print(f\"n10_params:\\n{n10_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LD3 Timesteps Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "import os\n",
    "\n",
    "from dataset import load_data_from_dir\n",
    "from trainer import LD3Trainer, ModelConfig, TrainingConfig, DiscretizeModelWrapper\n",
    "from utils import (\n",
    "    get_solvers,\n",
    "    parse_arguments,\n",
    "    adjust_hyper,\n",
    "    set_seed_everything,\n",
    "    move_tensor_to_device\n",
    ")\n",
    "from models import prepare_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_params(params: torch.tensor) -> float:    \n",
    "    start_time = time.time()\n",
    "    args = parse_arguments([\n",
    "        \"--all_config\", \"configs/cifar10.yml\",\n",
    "        \"--data_dir\", \"train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0\",\n",
    "        \"--num_train\", \"0\",\n",
    "        \"--num_valid\", \"50\",\n",
    "        \"--steps\", str(len(params)-1),\n",
    "        \"--training_rounds_v1\", \"1\",\n",
    "        \"--seed\", \"0\",\n",
    "    ])\n",
    "\n",
    "    set_seed_everything(args.seed)\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    wrapped_model, _, decoding_fn, noise_schedule, latent_resolution, latent_channel, _, _ = prepare_stuff(args)\n",
    "    adjust_hyper(args, latent_resolution, latent_channel)\n",
    "    solver, steps, solver_extra_params = get_solvers(\n",
    "        args.solver_name,\n",
    "        NFEs=args.steps,\n",
    "        order=args.order,\n",
    "        noise_schedule=noise_schedule,\n",
    "        unipc_variant=args.unipc_variant,\n",
    "    )\n",
    "    latents, targets, _, _, _ = load_data_from_dir( #this is what we take from trainig, targets are original images and latens latent goal\n",
    "        data_folder=args.data_dir, limit=args.num_train + args.num_valid, use_optimal_params=False\n",
    "    )\n",
    "\n",
    "    training_config = TrainingConfig(\n",
    "        train_data=latents,\n",
    "        valid_data=latents,\n",
    "        train_batch_size=args.main_train_batch_size,\n",
    "        valid_batch_size=args.main_valid_batch_size,\n",
    "        lr_time_1=args.lr_time_1,\n",
    "        shift_lr=args.shift_lr,\n",
    "        shift_lr_decay=args.shift_lr_decay,\n",
    "        min_lr_time_1=args.min_lr_time_1,\n",
    "        win_rate=args.win_rate,\n",
    "        patient=args.patient,\n",
    "        lr_time_decay=args.lr_time_decay,\n",
    "        momentum_time_1=args.momentum_time_1,\n",
    "        weight_decay_time_1=args.weight_decay_time_1,\n",
    "        loss_type=args.loss_type,\n",
    "        visualize=args.visualize,\n",
    "        no_v1=args.no_v1,\n",
    "        prior_timesteps=args.gits_ts,\n",
    "        match_prior=args.match_prior,\n",
    "    )\n",
    "    model_config = ModelConfig(\n",
    "        net=wrapped_model,\n",
    "        decoding_fn=decoding_fn,\n",
    "        noise_schedule=noise_schedule,\n",
    "        solver=solver,\n",
    "        solver_name=args.solver_name,\n",
    "        order=args.order,\n",
    "        steps=steps,\n",
    "        prior_bound=args.prior_bound,\n",
    "        resolution=latent_resolution,\n",
    "        channels=latent_channel,\n",
    "        time_mode=args.time_mode,\n",
    "        solver_extra_params=solver_extra_params,\n",
    "        device=device,\n",
    "    )\n",
    "    trainer = LD3Trainer(model_config, training_config)\n",
    "    dis_model = DiscretizeModelWrapper( #Changed through LTT\n",
    "            lambda_max=trainer.lambda_max,\n",
    "            lambda_min=trainer.lambda_min,\n",
    "            noise_schedule=trainer.noise_schedule,\n",
    "            time_mode = trainer.time_mode,\n",
    "        )\n",
    "    loss_list = torch.zeros(len(targets))\n",
    "    for i, (img, latent) in enumerate(zip(targets, latents)):\n",
    "    \n",
    "        img, latent = move_tensor_to_device(img, latent, device = device)\n",
    "        \n",
    "        timestep = dis_model.convert(params.unsqueeze(0))\n",
    "\n",
    "        x_next = trainer.noise_schedule.prior_transformation(latent)\n",
    "        x_next = trainer.solver.sample_simple(\n",
    "            model_fn=trainer.net,\n",
    "            x=x_next,\n",
    "            timesteps=timestep[0],\n",
    "            order=trainer.order,\n",
    "            NFEs=trainer.steps,\n",
    "            **trainer.solver_extra_params,\n",
    "            )\n",
    "        x_next = trainer.decoding_fn(x_next)\n",
    "        trainer.loss_vector = trainer.loss_fn(img.float(), x_next.float()).squeeze()\n",
    "        loss = trainer.loss_vector.mean() \n",
    "        loss_list[i] = loss\n",
    "    print(\"Time taken: \", time.time() - start_time)\n",
    "    return loss_list.mean().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for name, params in zip([\"n3\", \"n5\", \"n7\", \"n10\"], [n3_params, n5_params, n7_params, n10_params]):\n",
    "    print(f\"Loss for {name}: {evaluate_params(params)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/N10-val50-train450-rv12-seed0/final_ltt_model.pt\", 10\n",
    "model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/N10-val50-train50-rv12-seed0/ltt_model.pt\", 10\n",
    "\n",
    "model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/N10-val50-train450-rv12-seed0-fixed_scaling/final_ltt_model.pt\", 10\n",
    "model_path, steps  = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/N7-val50-train450-rv12-seed0-fixed_scaling/final_ltt_model.pt\", 7\n",
    "model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/N5-val50-train450-rv12-seed0-fixed_scaling/final_ltt_model.pt\", 5\n",
    "# model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/N3-val50-train450-rv12-seed0-fixed_scaling/final_ltt_model.pt\", 3\n",
    "model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/LTT_batch3_moreData_N5-val50-train450-r5/final_ltt_model.pt\", 5\n",
    "without_dropout_model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/LTT_after_ltt_change_batch1_without_dropout_N5-val50-train450-r10/final_ltt_model.pt\", 5\n",
    "with_dropout_model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/LTT_after_ltt_change_batch1_with_dropout_N5-val50-train450-r10/final_ltt_model.pt\", 5\n",
    "trained_on_optimal_without_dropout, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/runs/RandomModels/model_lr0.0001_batch5_without_dropout.pth\", 5\n",
    "after_ltt_change, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/LTT_After_LTT_DatasetAdjustement_batch3_N5-val50-train450-r2/final_ltt_model.pt\", 5\n",
    "\n",
    "ltt_model = LTT_model(steps=steps)\n",
    "state_dict = torch.load(trained_on_optimal_without_dropout, weights_only=True)\n",
    "ltt_model.load_state_dict(state_dict)  # Load the model state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(ltt_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = ltt_model.forward(torch.stack(second_outputs)) \n",
    "\n",
    "#visualize as violin plot over each of the 10 timesteps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "params_list = params_list.detach().numpy()\n",
    "params_list = params_list.reshape(-1, steps+1)\n",
    "params_list = pd.DataFrame(params_list, columns=[f\"{i}\" for i in range(steps+1)])\n",
    "sns.violinplot(data=params_list)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-net encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the output\n",
    "hook_storage = {}\n",
    "\n",
    "# Define the hook function using a closure\n",
    "def get_hook(storage):\n",
    "    def hook_fn(module, input, output):\n",
    "        storage[\"unet_output\"] = output  # Store output in the dictionary\n",
    "    return hook_fn\n",
    "# Register the hook on the UNet\n",
    "hook_handle = ltt_model.unet.register_forward_hook(get_hook(hook_storage))\n",
    "\n",
    "# Run the forward pass\n",
    "output = ltt_model.forward(latents[0].unsqueeze(0))\n",
    "\n",
    "# Retrieve the stored UNet output\n",
    "unet_output = hook_storage[\"unet_output\"]\n",
    "print(\"Stored UNet Output:\", unet_output.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(tensor1, tensor2):\n",
    "    return torch.mean((tensor1 - tensor2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = []\n",
    "num_matrices = 20\n",
    "for latent in latents[:num_matrices]:\n",
    "    output = ltt_model.forward(latent.unsqueeze(0))\n",
    "    encodings.append(hook_storage[\"unet_output\"])\n",
    "\n",
    "\n",
    "mse_matrix = np.zeros((num_matrices, num_matrices))\n",
    "\n",
    "for i in range(num_matrices):\n",
    "    for j in range(num_matrices):\n",
    "        mse_matrix[i, j] = mse(encodings[i], encodings[j])\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(mse_matrix, annot=False, cmap=\"viridis\", linewidths=0.5)\n",
    "plt.title(\"Pairwise MSE Heatmap\")\n",
    "plt.xlabel(\"Matrix Index\")\n",
    "plt.ylabel(\"Matrix Index\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Timesteps Per Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from torch.save(loss_matrix, os.path.join(args.data_dir, f\"loss_matrix.pt\"))\n",
    "optimal_dir = os.path.join(path, \"OldOptimSteps\")\n",
    "# loss_matrix = torch.load(os.path.join(data_dir, f\"loss_matrix.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(loss_matrix, annot=False, cmap=\"viridis\", linewidths=0.5)\n",
    "plt.title(\"Pairwise MSE Heatmap\")\n",
    "plt.xlabel(\"Matrix Index\")\n",
    "plt.ylabel(\"Matrix Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find lowest loss matrix\n",
    "min_loss = torch.min(loss_matrix, axis=1)\n",
    "min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(2):\n",
    "    min_values = [torch.min(loss_matrix[i, :j+1,]).item() for j in range(300)]\n",
    "    plt.plot(range(300), min_values, label=f'Line {i+1}')\n",
    "plt.title(\"Lowest Value of Second Dimension Up to That Point\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Lowest Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_matrix = torch.load(os.path.join(data_dir, f\"loss_grad_matrix.pt\"))\n",
    "gradient_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(gradient_matrix[2], annot=False, cmap=\"viridis\", linewidths=0.5)\n",
    "plt.title(\"Pairwise MSE Heatmap\")\n",
    "plt.xlabel(\"Matrix Index\")\n",
    "plt.ylabel(\"Matrix Index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(2):\n",
    "    plt.plot(range(300), abs(gradient_matrix[1, :, i]), label=f'Line {i+1}')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Gradient Matrix Lines\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Gradient Value (log scale)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.tensor([1,2,3,4,5,6], dtype=torch.float32)\n",
    "test_tensor2 = torch.tensor([1,2,3,4,5,6], dtype=torch.float32)*2\n",
    "test_tensor3 = torch.tensor([1,2,3,4,5,6], dtype=torch.float32)+1\n",
    "\n",
    "m = nn.Softmax()\n",
    "print(m(test_tensor))\n",
    "print(m(test_tensor2))\n",
    "print(m(test_tensor3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare optimal params for different images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_files = [f for f in os.listdir(path) if \"optimal_params\" in f]\n",
    "for i, file_name in enumerate(sorted(pt_files)[:]): #load all training files previously created\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    params, loss = torch.load(file_path, weights_only=True)\n",
    "    data = params.detach().numpy()\n",
    "    losses = loss.detach().numpy()\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    # Apply PCA to reduce to 2 dimensions\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(data_scaled)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(principal_components[:, 0], principal_components[:, 1],\n",
    "                        c=losses, cmap='viridis', edgecolor='k', s=100)\n",
    "    plt.colorbar(scatter, label='Loss')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('PCA Visualization Colored by Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(\"params\\n\", data)\n",
    "    print(\"losses\\n\", losses)\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_files = [f for f in os.listdir(path) if \"optimal_params\" in f]\n",
    "pt_files = sorted(pt_files)[:6]  # Load first 6 training files\n",
    "\n",
    "shapes = ['circle', 'square', 'diamond', 'triangle-up', 'triangle-down', 'cross']  # Different marker shapes\n",
    "\n",
    "all_data = []\n",
    "best_losses = []\n",
    "\n",
    "\n",
    "\n",
    "for i, file_name in enumerate(pt_files):\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    # Load parameters and loss\n",
    "    params, loss = torch.load(file_path, weights_only=True)  \n",
    "    data = params.detach().numpy()\n",
    "    losses = loss.detach().numpy()\n",
    "    all_data.append(data)\n",
    "    best_losses.append(losses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_data = np.stack(all_data).reshape(-1, steps+1)\n",
    "best_losses = np.stack(best_losses).reshape(-1)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(all_data)\n",
    "    \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(principal_components, columns=[\"PC1\", \"PC2\"])\n",
    "df[\"Loss\"] = best_losses\n",
    "df[\"Shape\"] = [x for x in shapes for _ in range(10)]\n",
    "df[\"params\"] = list(all_data)\n",
    "df[\"params\"] = df[\"params\"].apply(lambda x: [round(v, 4) for v in x])\n",
    "\n",
    "# # Create interactive 3D scatter plot using Plotly\n",
    "fig = px.scatter(df, x=\"PC1\", y=\"PC2\", color=\"Loss\", symbol=\"Shape\",\n",
    "                    hover_data={\"Loss\": \":.4f\", \"PC1\": False, \"PC2\": False, \"params\": True},\n",
    "                    color_continuous_scale=\"viridis\",\n",
    "                    title=\"2D PCA Visualization Colored by Loss with Different Shapes\")\n",
    "\n",
    "# Move color bar to the left\n",
    "fig.update_layout(coloraxis_colorbar=dict(x=-0.2))  \n",
    "\n",
    "fig.update_traces(marker=dict(size=5, line=dict(width=1, color=\"black\")))\n",
    "\n",
    "# Show interactive 3D plot\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_files = [f for f in os.listdir(path) if \"optimal_params\" in f]\n",
    "pt_files = sorted(pt_files)[:6]  # Load first 6 training files\n",
    "\n",
    "shapes = ['circle', 'square', 'diamond', 'triangle-up', 'triangle-down', 'cross']  # Different marker shapes\n",
    "\n",
    "all_data = []\n",
    "best_losses = []\n",
    "\n",
    "\n",
    "\n",
    "for i, file_name in enumerate(pt_files):\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    # Load parameters and loss\n",
    "    params, loss = torch.load(file_path, weights_only=True)  \n",
    "    data = params.detach().numpy()\n",
    "    losses = loss.detach().numpy()\n",
    "    all_data.append(data)\n",
    "    best_losses.append(losses)\n",
    "\n",
    "\n",
    "all_data = np.stack(all_data).reshape(-1, steps+1)\n",
    "best_losses = np.stack(best_losses).reshape(-1)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "principal_components = pca.fit_transform(all_data)\n",
    "    \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(principal_components, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "df[\"Loss\"] = best_losses\n",
    "df[\"Shape\"] = [x for x in shapes for _ in range(10)]\n",
    "df[\"params\"] = list(all_data)\n",
    "df[\"params\"] = df[\"params\"].apply(lambda x: [round(v, 4) for v in x])\n",
    "\n",
    "# # Create interactive 3D scatter plot using Plotly\n",
    "fig = px.scatter_3d(df, x=\"PC1\", y=\"PC2\", z=\"PC3\", color=\"Loss\", symbol=\"Shape\",\n",
    "                    hover_data={\"Loss\": \":.4f\", \"PC1\": False, \"PC2\": False, \"PC3\": False, \"params\": True},\n",
    "                    color_continuous_scale=\"viridis\",\n",
    "                    title=\"3D PCA Visualization Colored by Loss with Different Shapes\")\n",
    "\n",
    "# Move color bar to the left\n",
    "fig.update_layout(coloraxis_colorbar=dict(x=-0.2))  \n",
    "\n",
    "fig.update_traces(marker=dict(size=5, line=dict(width=1, color=\"black\")))\n",
    "\n",
    "# Show interactive 3D plot\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.histplot(best_losses, bins=50, kde=True)\n",
    "plt.title(\"Loss Distribution\")\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only best trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_files = [f for f in os.listdir(optimal_dir) if \"optimal_params\" in f]\n",
    "pt_files = sorted(pt_files) # Load first 6 training files\n",
    "\n",
    "all_data = []\n",
    "best_losses = []\n",
    "all_losses = []\n",
    "\n",
    "for i, file_name in enumerate(pt_files):\n",
    "    file_path = os.path.join(optimal_dir, file_name)\n",
    "    # Load parameters and loss\n",
    "    params, loss = torch.load(file_path, weights_only=True)  \n",
    "    data = params.detach().numpy()[0]\n",
    "    losses = loss.detach().numpy()\n",
    "    all_data.append(data)\n",
    "    best_losses.append(losses[0])\n",
    "    all_losses.append(losses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_data = np.stack(all_data).reshape(-1, steps+1)\n",
    "best_losses = np.stack(best_losses).reshape(-1)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "principal_components = pca.fit_transform(all_data)\n",
    "    \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(principal_components, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "df[\"Loss\"] = best_losses\n",
    "df[\"params\"] = list(all_data)\n",
    "df[\"params\"] = df[\"params\"].apply(lambda x: [round(v, 4) for v in x])\n",
    "\n",
    "# # Create interactive 3D scatter plot using Plotly\n",
    "fig = px.scatter_3d(df, x=\"PC1\", y=\"PC2\", z=\"PC3\", color=\"Loss\",\n",
    "                    hover_data={\"Loss\": \":.4f\", \"PC1\": False, \"PC2\": False, \"PC3\": False, \"params\": True},\n",
    "                    color_continuous_scale=\"viridis\",\n",
    "                    title=\"3D PCA Visualization Colored by Loss with Different Shapes\")\n",
    "\n",
    "# Move color bar to the left\n",
    "fig.update_layout(coloraxis_colorbar=dict(x=-0.2))  \n",
    "\n",
    "fig.update_traces(marker=dict(size=5, line=dict(width=1, color=\"black\")))\n",
    "\n",
    "# Show interactive 3D plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss distribution of all_losses\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.histplot(best_losses, bins=50, kde=True)\n",
    "plt.title(\"Loss Distribution\")\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming all_losses and best_losses are defined\n",
    "all_losses = np.stack(all_losses).reshape(-1)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add histogram for all_losses\n",
    "fig.add_trace(go.Histogram(x=all_losses, nbinsx=75, histnorm='probability density', \n",
    "                           name=\"All Losses\", marker_color='blue', opacity=0.5))\n",
    "\n",
    "# Add histogram for best_losses\n",
    "fig.add_trace(go.Histogram(x=best_losses, nbinsx=75, histnorm='probability density', \n",
    "                           name=\"Best Losses\", marker_color='red', opacity=0.5))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Loss Distribution\",\n",
    "    xaxis_title=\"Loss\",\n",
    "    yaxis_title=\"Density\",\n",
    "    barmode='overlay',  # Overlay both histograms\n",
    "    template=\"plotly_white\"  # Optional: use a clean background\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print(f\"Mean lost of first 50 validation losses: {np.mean(all_losses[:500]):.4f}\")\n",
    "print(f\"Mean lost of 50 validation best losses: {np.mean(best_losses[:50]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New generated ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0/optimal_params_000003_N10_steps5.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With 30 iterations and only 1 trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_t_dir = \"/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0/train/opt_t\"\n",
    "opt_t_files = [f for f in os.listdir(opt_t_dir) if f.endswith('.pth')]\n",
    "opt_t_files = sorted(opt_t_files) \n",
    "\n",
    "\n",
    "loss_np = np.zeros((len(opt_t_files)))\n",
    "\n",
    "for i,file_path in enumerate(opt_t_files):\n",
    "    opt_t_path = os.path.join(opt_t_dir, file_path)\n",
    "    opt_t = torch.load(opt_t_path, weights_only=True)[1]\n",
    "    loss_np[i] = opt_t\n",
    "\n",
    "\n",
    "print(f\"Mean loss of optimal train timesteps: {np.mean(loss_np):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_t_dir = \"/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0/validation/opt_t\"\n",
    "opt_t_files = [f for f in os.listdir(opt_t_dir) if f.endswith('.pth')]\n",
    "opt_t_files = sorted(opt_t_files) \n",
    "\n",
    "\n",
    "loss_np = np.zeros((len(opt_t_files)))\n",
    "\n",
    "for i,file_path in enumerate(opt_t_files):\n",
    "    opt_t_path = os.path.join(opt_t_dir, file_path)\n",
    "    opt_t = torch.load(opt_t_path, weights_only=True)[1]\n",
    "    loss_np[i] = opt_t\n",
    "\n",
    "\n",
    "print(f\"Mean loss of optimal validation timesteps: {np.mean(loss_np):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 50 iterations and 3 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_t_dir = \"/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0/train/opt_t_clever_initialisation\"\n",
    "opt_t_files = [f for f in os.listdir(opt_t_dir) if f.endswith('.pth')]\n",
    "opt_t_files = sorted(opt_t_files) \n",
    "\n",
    "\n",
    "loss_np = np.zeros((len(opt_t_files)))\n",
    "\n",
    "for i,file_path in enumerate(opt_t_files):\n",
    "    opt_t_path = os.path.join(opt_t_dir, file_path)\n",
    "    opt_t = torch.load(opt_t_path, weights_only=True)[1][0]\n",
    "    loss_np[i] = opt_t\n",
    "\n",
    "\n",
    "print(f\"Mean loss of optimal train timesteps: {np.mean(loss_np):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_t_dir = \"/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0/validation/opt_t_clever_initialisation\"\n",
    "opt_t_files = [f for f in os.listdir(opt_t_dir) if f.endswith('.pth')]\n",
    "opt_t_files = sorted(opt_t_files) \n",
    "\n",
    "\n",
    "loss_np = np.zeros((len(opt_t_files)))\n",
    "\n",
    "for i,file_path in enumerate(opt_t_files):\n",
    "    opt_t_path = os.path.join(opt_t_dir, file_path)\n",
    "    opt_t = torch.load(opt_t_path, weights_only=True)[1][0]\n",
    "    loss_np[i] = opt_t\n",
    "\n",
    "\n",
    "print(f\"Mean loss of optimal validation timesteps: {np.mean(loss_np):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating latent -> image pairs efficiently in mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the PNG image\n",
    "image_path = '/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0/train/img/000000.png'\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Define a transform to convert the image to a tensor\n",
    "transform_to_tensor = transforms.ToTensor()\n",
    "\n",
    "# Apply the transform to the image\n",
    "tensor_from_img = transform_to_tensor(image)\n",
    "\n",
    "# Verify the tensor shape and type\n",
    "print(f\"Tensor shape: {tensor_from_img.shape}\")  # Should print: torch.Size([C, H, W])\n",
    "print(f\"Tensor dtype: {tensor_from_img.dtype}\")  # Should print: torch.float32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def png_to_tensor(image):\n",
    "    # Load image\n",
    "\n",
    "    # Convert to NumPy array and normalize (0-255 → 0-1)\n",
    "    image_np = np.array(image, dtype=np.float32) / 255.0\n",
    "\n",
    "    # Reorder dimensions (H, W, C) → (C, H, W)\n",
    "    image_tensor = torch.tensor(image_np).permute(2, 0, 1)\n",
    "\n",
    "    # Reverse normalization ((x * 2) - 1)\n",
    "    image_tensor = (image_tensor * 2.0) - 1.0  \n",
    "\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_original = torch.load(\"/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0/test_images/img_000000.pt\")\n",
    "print(f\"Tensor shape: {tensor_original.shape}\")  # Should print: torch.Size([C, H, W])\n",
    "print(f\"Tensor dtype: {tensor_original.dtype}\")  # Should print: torch.float32\n",
    "\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "image_pil = to_pil((tensor_original +1 ) / 2)\n",
    "# image_pil = to_pil(torch.clip((tensor_original +1 ) / 2, 0,1))\n",
    "plt.imshow(image_pil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = transform_to_tensor(image_pil)\n",
    "img_tensor = (img_tensor - 0.5) * 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(img_tensor - tensor_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Convert the tensor to a numpy array and transpose the dimensions to match the image format\n",
    "tensor_original_np = tensor_original.permute(1, 2, 0).numpy()\n",
    "\n",
    "# Convert the tensor values from the range [-1, 1] to [0, 255]\n",
    "tensor_original_np = ((tensor_original_np + 1) * 127.5).astype(np.uint8)\n",
    "\n",
    "# Create a PIL image from the numpy array\n",
    "image_pil = Image.fromarray(tensor_original_np)\n",
    "\n",
    "# Save the image\n",
    "image_pil.save(\"/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0/test_images/image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(png_to_tensor(image) - torch.clip(tensor_original, -1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.transpose(tensor_original, (1, 2, 0))\n",
    "convert =  lambda x: (x + 1.0) / 2.0\n",
    "image = convert(image)\n",
    "\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")  # Hide axes\n",
    "plt.savefig(\"/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0/test_images/original_image.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Create a random tensor [3, 32, 32] with values in range [-1, 1]\n",
    "original_tensor = torch.rand(3, 32, 32) * 2 - 1  # Scale to [-1, 1]\n",
    "\n",
    "# Convert tensor to image\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = (tensor + 1.0) / 2.0  # Scale from [-1, 1] to [0, 1]\n",
    "    tensor = torch.clamp(tensor, 0, 1)  # Ensure valid range\n",
    "    image_np = (tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "    return Image.fromarray(image_np, \"RGB\")\n",
    "\n",
    "# Convert image back to tensor\n",
    "def image_to_tensor(image):\n",
    "    image_np = np.array(image).astype(np.float32) / 255.0  # Convert to [0,1]\n",
    "    tensor = torch.tensor(image_np).permute(2, 0, 1)  # Convert to [C, H, W]\n",
    "    tensor = (tensor * 2.0) - 1.0  # Scale back to [-1, 1]\n",
    "    return tensor\n",
    "\n",
    "# Convert tensor to image and display it\n",
    "image = tensor_to_image(original_tensor)\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Convert image back to tensor\n",
    "recovered_tensor = image_to_tensor(image)\n",
    "\n",
    "# Calculate difference\n",
    "difference = torch.abs(original_tensor - recovered_tensor)\n",
    "error = torch.mean(difference).item()  # Mean absolute error\n",
    "\n",
    "print(f\"Mean Absolute Error between original and recovered tensor: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(tensor: torch.Tensor) -> None:\n",
    "    convert =  lambda x: (x + 1.0) / 2.0\n",
    "    samples_raw = convert(tensor.unsqueeze(0))\n",
    "    samples = np.clip(  #10 because of batch size\n",
    "                    samples_raw.permute(0, 2, 3, 1).cpu().numpy() * 255.0, 0, 255\n",
    "                ).astype(np.uint8)\n",
    "    image_np = samples.reshape((-1, 32, 32, 3))[0]\n",
    "\n",
    "\n",
    "\n",
    "    plt.imshow(image_np)\n",
    "    plt.title('Generated Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# PIL.Image.fromarray(image_np, \"RGB\").save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_original_np = tensor_original.detach().cpu().numpy()\n",
    "tensor_from_img_np = tensor_from_img.detach().cpu().numpy()\n",
    "undo_convert = lambda x: (x * 2.0) - 1.0\n",
    "tensor_from_img_np = undo_convert(tensor_from_img_np)\n",
    "tensor_original_np = np.clip(tensor_original_np, 0, 1)\n",
    "tensor_from_img_np = np.clip(tensor_from_img_np, 0, 1)\n",
    "np.sum(tensor_original_np - tensor_from_img_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original tensor image and the tensor from the loaded image side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Convert tensors to numpy arrays for plotting\n",
    "tensor_original_np = tensor_original.permute(1, 2, 0).numpy()\n",
    "tensor_from_img_np = tensor_from_img.permute(1, 2, 0).numpy()\n",
    "undo_convert = lambda x: (x * 2.0) - 1.0\n",
    "tensor_from_img_np = undo_convert(tensor_from_img_np)\n",
    "\n",
    "# Plot the original tensor image\n",
    "axes[0].imshow(tensor_original_np)\n",
    "axes[0].set_title('Original Tensor Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot the tensor from the loaded image\n",
    "axes[1].imshow(tensor_from_img_np)\n",
    "axes[1].set_title('Tensor from Loaded Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(tensor_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import LTTDataset\n",
    "path = \"/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0/validation\"\n",
    "dataset = LTTDataset(path)\n",
    "\n",
    "for i in range(1):\n",
    "    img, latent, opt_t = dataset[i]\n",
    "    # Convert the tensor to a numpy array and transpose the dimensions to match the image format\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    latent_np = latent.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Plot the image and latent side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axes[0].imshow(img_np)\n",
    "    axes[0].set_title(f'Image {i}')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(latent_np)\n",
    "    axes[1].set_title(f'Latent {i}')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    print(opt_t)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# the second output should be exactly the same proving that the data generator working\n",
    "for i in range(1):\n",
    "    img, latent, _ = dataset[i]\n",
    "    # Convert the tensor to a numpy array and transpose the dimensions to match the image format\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    latent_np = latent.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Plot the image and latent side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axes[0].imshow(img_np)\n",
    "    axes[0].set_title(f'Image {i}')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(latent_np)\n",
    "    axes[1].set_title(f'Latent {i}')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import LTTDataset\n",
    "path = \"/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0/train/img\"\n",
    "dataset = LTTDataset(path)\n",
    "\n",
    "for i in range(1):\n",
    "    img, latent = dataset[i]\n",
    "    # Convert the tensor to a numpy array and transpose the dimensions to match the image format\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    latent_np = latent.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Plot the image and latent side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axes[0].imshow(img_np)\n",
    "    axes[0].set_title(f'Image {i}')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(latent_np)\n",
    "    axes[1].set_title(f'Latent {i}')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# the second output should be exactly the same proving that the data generator working correctly\n",
    "for i in range(1):\n",
    "    img, latent = dataset[i]\n",
    "    # Convert the tensor to a numpy array and transpose the dimensions to match the image format\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    latent_np = latent.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Plot the image and latent side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axes[0].imshow(img_np)\n",
    "    axes[0].set_title(f'Image {i}')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(latent_np)\n",
    "    axes[1].set_title(f'Latent {i}')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Timestep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter  # Add this import\n",
    "import lpips\n",
    "from trainer import LD3Trainer, ModelConfig, TrainingConfig, DiscretizeModelWrapper\n",
    "from utils import get_solvers, move_tensor_to_device, parse_arguments, set_seed_everything\n",
    "\n",
    "from dataset import load_data_from_dir, LTTDataset\n",
    "from latent_to_timestep_model import  Delta_LTT_model\n",
    "from models import prepare_stuff\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from utils import visual\n",
    "\n",
    "\n",
    "args = parse_arguments([\n",
    "    \"--all_config\", \"configs/cifar10.yml\",\n",
    "    \"--data_dir\", \"train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0\",\n",
    "    \"--num_train\", \"1000\",\n",
    "    \"--num_valid\", \"1000\",\n",
    "    \"--main_train_batch_size\", \"200\",\n",
    "    \"--main_valid_batch_size\", \"200\",\n",
    "    \"--training_rounds_v1\", \"1\",\n",
    "    \"--log_path\", \"logs/logs_cifar10\",\n",
    "    \"--force_train\", \"True\",\n",
    "    \"--steps\", \"5\",\n",
    "    \"--lr_time_1\", \"0.00005\",\n",
    "    \"--mlp_dropout\", \"0.0\",\n",
    "    \"--log_suffix\", \"BiggerValidation_GroupNorm_EvalTrue\"\n",
    "])\n",
    "\n",
    "set_seed_everything(args.seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Dataset\n",
    "data_dir = 'train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0'\n",
    "model_dir = \"runs_delta_timesteps/models\"\n",
    "steps = 5\n",
    "lpips_loss_fn = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "\n",
    "wrapped_model, _, decoding_fn, noise_schedule, latent_resolution, latent_channel, _, _ = prepare_stuff(args)\n",
    "solver, steps, solver_extra_params = get_solvers(\n",
    "    args.solver_name,\n",
    "    NFEs=args.steps,\n",
    "    order=args.order,\n",
    "    noise_schedule=noise_schedule,\n",
    "    unipc_variant=args.unipc_variant,\n",
    ")\n",
    "\n",
    "order = args.order  \n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    collated_batch = []\n",
    "    for samples in zip(*batch):\n",
    "        if any(item is None for item in samples):\n",
    "            collated_batch.append(None)\n",
    "        else:\n",
    "            collated_batch.append(torch.utils.data._utils.collate.default_collate(samples))\n",
    "    return collated_batch\n",
    "\n",
    "valid_dataset = LTTDataset(dir=os.path.join(data_dir, \"validation\"), size=args.num_valid, train_flag=False, use_optimal_params=False) \n",
    "train_dataset = LTTDataset(dir=os.path.join(data_dir, \"train\"), size=args.num_train, train_flag=True, use_optimal_params=False)\n",
    "\n",
    "delta_ltt_model = Delta_LTT_model(steps = steps, mlp_dropout=args.mlp_dropout)\n",
    "delta_ltt_model = delta_ltt_model.to(device)\n",
    "\n",
    "wrapped_model, _, decoding_fn, noise_schedule, latent_resolution, latent_channel, _, _ = prepare_stuff(args)\n",
    "solver, steps, solver_extra_params = get_solvers(\n",
    "    args.solver_name,\n",
    "    NFEs=args.steps,\n",
    "    order=args.order,\n",
    "    noise_schedule=noise_schedule,\n",
    "    unipc_variant=args.unipc_variant,\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    train_data=train_dataset,\n",
    "    valid_data=valid_dataset,\n",
    "    train_batch_size=args.main_train_batch_size,\n",
    "    valid_batch_size=args.main_valid_batch_size,\n",
    "    lr_time_1=args.lr_time_1,\n",
    "    shift_lr=args.shift_lr,\n",
    "    shift_lr_decay=args.shift_lr_decay,\n",
    "    min_lr_time_1=args.min_lr_time_1,\n",
    "    win_rate=args.win_rate,\n",
    "    patient=args.patient,\n",
    "    lr_time_decay=args.lr_time_decay,\n",
    "    momentum_time_1=args.momentum_time_1,\n",
    "    weight_decay_time_1=args.weight_decay_time_1,\n",
    "    loss_type=args.loss_type,\n",
    "    visualize=args.visualize,\n",
    "    no_v1=args.no_v1,\n",
    "    prior_timesteps=args.gits_ts,\n",
    "    match_prior=args.match_prior,\n",
    ")\n",
    "model_config = ModelConfig(\n",
    "    net=wrapped_model,\n",
    "    decoding_fn=decoding_fn,\n",
    "    noise_schedule=noise_schedule,\n",
    "    solver=solver,\n",
    "    solver_name=args.solver_name,\n",
    "    order=args.order,\n",
    "    steps=steps,\n",
    "    prior_bound=args.prior_bound,\n",
    "    resolution=latent_resolution,\n",
    "    channels=latent_channel,\n",
    "    time_mode=args.time_mode,\n",
    "    solver_extra_params=solver_extra_params,\n",
    "    device=device,\n",
    ")\n",
    "trainer = LD3Trainer(model_config, training_config)\n",
    "\n",
    "\n",
    "dis_model = DiscretizeModelWrapper( #Changed through LTT\n",
    "        lambda_max=trainer.lambda_max,\n",
    "        lambda_min=trainer.lambda_min,\n",
    "        noise_schedule=trainer.noise_schedule,\n",
    "        time_mode = trainer.time_mode,\n",
    "    )\n",
    "\n",
    "\n",
    "img, latent, _ = valid_dataset[0]\n",
    "latent = latent.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved state dictionary\n",
    "\n",
    "group_norm_model = \"model_lr5e-05_batch3_nTrain500000_BiggerValidation_GroupNorm_EvalTrue\"\n",
    "rerun_alpha_3 = \"model_lr5e-05_batch3_nTrain500000_RerunAlpha\"\n",
    "rerun_alpha_30 = \"model_lr5e-05_batch30_nTrain500000_RerunAlpha\"\n",
    "state_dict = torch.load(os.path.join(model_dir, rerun_alpha_30), map_location=device,weights_only=True)\n",
    "\n",
    "# Load the state dictionary into the delta_ltt_model\n",
    "delta_ltt_model.load_state_dict(state_dict)\n",
    "\n",
    "print(\"Parameters successfully loaded into delta_ltt_model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    delta_ltt_model.eval()\n",
    "    for i,batch in enumerate(trainer.train_loader):\n",
    "        img, latent, _ = batch\n",
    "        latent = latent.to(device)\n",
    "        img = img.to(device)\n",
    "\n",
    "        x_next_list = trainer.noise_schedule.prior_transformation(latent) #Multiply with timestep in edm case (x80 in beginning)\n",
    "        x_next_computed = []\n",
    "        x_next_list_computed = []\n",
    "        for x in x_next_list:\n",
    "            x_next, x_list, _ = trainer.solver.delta_sample_simple(\n",
    "                model_fn=trainer.net,\n",
    "                delta_ltt=delta_ltt_model,\n",
    "                x=x.unsqueeze(0),\n",
    "                order=trainer.order,\n",
    "                steps = trainer.steps,\n",
    "                start_timestep = 80,\n",
    "                NFEs=trainer.steps,\n",
    "                condition=None,\n",
    "                unconditional_condition=None,\n",
    "                **trainer.solver_extra_params,\n",
    "            )\n",
    "            x_next_computed.append(x_next)#This was wrong the whole time?\n",
    "        \n",
    "        x_next_computed = torch.cat(x_next_computed, dim=0) \n",
    "        loss_vector = trainer.loss_fn(img.float(), x_next_computed.float()).squeeze()\n",
    "        loss = loss_vector.mean()\n",
    "        print(f\"Validated on iter{i}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valdiation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_timesteps = np.zeros((args.num_valid, steps+1))\n",
    "all_losses = np.zeros((args.num_valid) // args.main_valid_batch_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "    delta_ltt_model.eval()\n",
    "    for i,batch in enumerate(trainer.valid_only_loader):\n",
    "        img, latent, _ = batch\n",
    "        latent = latent.to(device)\n",
    "        img = img.to(device)\n",
    "\n",
    "        x_next_list = trainer.noise_schedule.prior_transformation(latent) #Multiply with timestep in edm case (x80 in beginning)\n",
    "        x_next_computed = []\n",
    "        x_next_list_computed = []\n",
    "        for j, x in enumerate(x_next_list):\n",
    "            x_next, x_list, t_list = trainer.solver.delta_sample_simple(\n",
    "                model_fn=trainer.net,\n",
    "                delta_ltt=delta_ltt_model,\n",
    "                x=x.unsqueeze(0),\n",
    "                order=trainer.order,\n",
    "                steps = trainer.steps,\n",
    "                start_timestep = 80,\n",
    "                NFEs=trainer.steps,\n",
    "                condition=None,\n",
    "                unconditional_condition=None,\n",
    "                **trainer.solver_extra_params,\n",
    "            )\n",
    "            x_next_computed.append(x_next)#This was wrong the whole time?\n",
    "            all_timesteps[i*args.main_valid_batch_size+j] = t_list\n",
    "        \n",
    "        x_next_computed = torch.cat(x_next_computed, dim=0) \n",
    "        loss_vector = trainer.loss_fn(img.float(), x_next_computed.float()).squeeze()\n",
    "        loss = loss_vector.mean()\n",
    "        all_losses[i] = loss.item()\n",
    "        print(f\"Validated on iter{i}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Loss: {np.mean(all_losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestep Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram for each column in all_timesteps\n",
    "num_columns = all_timesteps.shape[1]\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i in range(num_columns):\n",
    "    plt.subplot(2, (num_columns + 1) // 2, i + 1)  # Arrange subplots in rows\n",
    "    plt.hist(all_timesteps[:, i], bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.title(f\"Timestep {i}\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence if Prior Timestep and Steps Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratios = np.zeros((80))\n",
    "for t in range(0,80):\n",
    "    delta_timestep_ratio = delta_ltt_model(latent.unsqueeze(0), torch.tensor(t, device=device), torch.tensor(5, device=device))\n",
    "    all_ratios[t] = delta_timestep_ratio.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(all_ratios)), all_ratios, color='blue', alpha=0.7)\n",
    "plt.title(\"All Ratios at differnt timestep with same latent\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Ratio\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime of Diffusion Model vs DLTT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = valid_dataset[0][1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from noise_schedulers import NoiseScheduleVE\n",
    "from models.edm_uncond import model_wrapper\n",
    "with open(\"pretrained/edm-cifar10-32x32-uncond-vp.pkl\", \"rb\") as f:\n",
    "    net = pickle.load(f)[\"ema\"].to(device)\n",
    "noise_schedule = NoiseScheduleVE(schedule='edm')\n",
    "\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_fn = model_wrapper(net, noise_schedule)\n",
    "\n",
    "x = latent.unsqueeze(0)\n",
    "t = torch.tensor(80, device = device)\n",
    "model_fn(x, t.expand((x.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(x, t.expand((x.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack 10000 latens and run them through while measuring time\n",
    "import time\n",
    "\n",
    "all_latents = torch.stack([latent for _ in range(1000)])\n",
    "all_latents = all_latents.to(device)\n",
    "t = torch.tensor(80, device = device)\n",
    "start = time.time()\n",
    "model_fn(all_latents, t.expand((all_latents.shape[0])))\n",
    "end = time.time()\n",
    "print(f\"Time taken for 1000 latents: {end-start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(80, device = device)\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    delta_ltt_model(all_latents, t, torch.tensor(5, device=x.device))\n",
    "end = time.time()\n",
    "print(f\"Time taken for 1000 latents: {(end-start) / 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.404688119888306 / 0.05439952373504639"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempting to get Bottleneck layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom wrapper to handle the additional argument\n",
    "class ModelSummaryWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ModelSummaryWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split the input tensor into the required inputs\n",
    "        batch_size = x.shape[0]\n",
    "        t = torch.tensor(80, device=x.device).expand(batch_size)  # Adjust the default value as needed\n",
    "        return self.model(x, t)\n",
    "\n",
    "# Wrap the model\n",
    "summary_wrapper = ModelSummaryWrapper(net)\n",
    "\n",
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(1, 3, 32, 32).to(device)  # Adjust dimensions as needed\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(summary_wrapper, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, layer) in enumerate(net.named_modules()):\n",
    "    print(f\"{i+1}: Layer Name: {name}, Layer Type: {type(layer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4278e+00,  6.5677e+00,  3.9757e+00,  6.1155e+00, -5.6272e-02,\n",
       "          4.7583e+00, -3.5497e+00,  1.2434e+00,  3.0584e+00, -2.9782e+00,\n",
       "          1.0482e+00,  3.8307e+00, -2.7746e+00,  4.4532e+00,  2.0363e+00,\n",
       "          2.6655e+00,  4.5572e-01,  6.7098e+00, -4.3862e+00, -2.5329e+00,\n",
       "          4.5053e-01, -5.6717e+00, -5.5204e+00,  6.4583e+00, -3.1902e-01,\n",
       "          7.1224e+00, -8.4573e-01,  3.0359e+00, -2.1529e+00, -2.4064e+00,\n",
       "          7.7525e-01,  5.5615e+00,  2.2871e+00,  3.2362e+00,  4.9820e+00,\n",
       "         -1.1608e+00,  2.2242e+00,  3.0207e+00,  1.5397e+00, -1.2069e+01,\n",
       "         -2.5013e+00,  3.8893e-01,  3.8898e+00,  5.8242e+00,  3.8508e+00,\n",
       "          4.8703e+00, -7.6412e+00,  3.8769e+00,  2.8533e+00, -7.9984e-01,\n",
       "          3.5449e+00,  1.0804e+00,  4.4722e+00,  5.4717e+00,  1.0273e+01,\n",
       "         -5.7373e+00,  7.9053e+00, -3.6632e+00,  1.0591e+00,  4.0612e+00,\n",
       "         -1.9804e+00,  1.2799e+00,  1.7201e+00,  3.8070e+00,  4.9658e+00,\n",
       "         -7.9767e+01,  1.3754e+00,  5.5996e+00,  5.9836e+00,  1.3044e+01,\n",
       "          4.6541e-01,  8.9888e+00,  5.6056e+00,  1.9793e+00,  5.9292e-01,\n",
       "          1.3246e+00,  1.1287e+00, -7.0958e+00,  4.5237e+00, -2.7926e-01,\n",
       "          4.0898e+00,  8.0513e+00, -7.2313e+00,  1.1313e+00,  1.3778e+00,\n",
       "          8.6409e+00,  3.2543e+00, -2.2631e+01,  5.2058e+00,  4.7077e+00,\n",
       "          4.3436e+00,  2.8280e+00,  4.0021e+00,  5.5438e+00, -1.1654e+01,\n",
       "          6.0235e+00, -8.8926e-01,  2.6336e+00,  4.3808e+00, -2.2018e-01,\n",
       "          5.1506e-01,  4.0632e-01,  3.3365e+00,  4.7290e+00, -5.7889e+00,\n",
       "          1.0956e+01,  5.6423e+00,  2.1270e+00,  4.6938e+00,  3.1620e+00,\n",
       "         -2.3176e+00,  4.7719e+00,  3.0210e-01, -1.1652e+00, -5.0870e-01,\n",
       "          3.7820e-01,  9.2536e+00,  6.5866e+00, -9.9618e-01, -9.2471e+00,\n",
       "         -3.1748e+00,  2.2165e+00,  6.5882e+00,  5.9111e+00, -2.5133e+00,\n",
       "          3.8533e+00, -5.4619e+00,  3.3433e+00,  4.2790e+00, -4.2837e-01,\n",
       "         -1.6457e+00, -1.8949e+00, -1.4851e+00, -4.3418e+00,  1.4986e+00,\n",
       "          1.6345e+00,  1.7415e+00,  6.3887e-01, -8.6527e-01, -3.4763e+00,\n",
       "          5.8633e+00, -2.3624e+00,  5.1139e+00,  5.4481e+00,  1.1101e-01,\n",
       "          1.5007e+00,  2.3305e+00, -1.9684e+00,  7.4733e+00,  4.7887e+00,\n",
       "         -2.1057e+00, -2.3988e-01, -1.0313e-01, -9.0017e-01,  6.8398e-01,\n",
       "          2.9513e+00,  2.6495e+00,  4.4906e+00,  1.6757e+00, -1.7537e+00,\n",
       "          3.9816e+00,  1.7867e+00,  3.4042e+00,  4.5309e+00,  2.3672e-01,\n",
       "         -2.7753e+00, -2.0862e+00,  3.3998e+00,  3.0647e+00,  3.4801e+00,\n",
       "         -3.7816e+00, -1.2380e+00,  3.6349e+00,  8.5576e+00,  3.8192e+00,\n",
       "          7.0070e+00, -3.9523e+00,  1.5492e+00,  1.9322e+00,  1.7831e+00,\n",
       "          2.7808e+00,  4.6528e+00,  1.1986e+00, -8.6681e-01, -5.8778e+00,\n",
       "         -2.7882e-02,  2.6326e+00,  5.9063e+00, -9.9052e-01,  1.8153e+00,\n",
       "          2.7337e+00,  3.1502e+00,  5.9497e+00,  4.2187e+00,  5.8122e+00,\n",
       "          6.3458e+00, -6.8564e-01,  5.0003e+00, -7.3221e+00,  1.8591e+00,\n",
       "          6.3804e+00,  1.1467e+01,  7.1729e+00,  8.5752e+00, -8.9240e+00,\n",
       "          1.0684e+01,  2.4720e+00, -5.2071e-01,  4.8725e+00,  3.4254e+00,\n",
       "          3.2558e+00,  1.4190e+00,  5.8161e-02,  2.0633e+00, -4.0472e-01,\n",
       "         -6.1560e-01,  3.6811e+00,  5.7932e-01,  6.1571e+00,  1.9648e-01,\n",
       "          3.4424e+00, -3.1474e+01,  2.8721e+00,  4.0006e+00,  2.4196e+00,\n",
       "          4.6642e+00,  2.5436e+00, -2.8763e+00,  2.9455e+00,  3.8998e+00,\n",
       "          1.3274e+00, -4.9976e+00, -3.0277e+01,  8.1389e+00,  1.1807e+01,\n",
       "          1.3951e-01,  4.1242e+00,  1.4603e+00,  1.9694e-01,  3.3511e+00,\n",
       "          6.2808e+00,  8.4674e-01,  9.1075e+00,  1.5471e-01,  3.4298e+00,\n",
       "          8.6316e+00, -4.1604e+00, -7.5327e+00,  6.9623e+00,  3.7527e+00,\n",
       "          2.3402e+00, -1.7965e+00,  4.9458e+00, -8.1720e-01, -2.0649e+00,\n",
       "         -4.6142e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = net.model\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    global bottleneck_output\n",
    "    bottleneck_output = output\n",
    "\n",
    "# Register the hook\n",
    "hook = model.enc[\"8x8_block3\"].affine.register_forward_hook(hook_fn)\n",
    "\n",
    "input_image = torch.randn(1, 3, 32, 32)\n",
    "output_image = model(x, t.expand((x.shape[0])), None)\n",
    "print(bottleneck_output.shape)\n",
    "\n",
    "hook.remove()\n",
    "bottleneck_output\n",
    "#model.enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Delta Ltt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize the SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/delta_ltt_model_visualization\")\n",
    "\n",
    "# Add the model graph to TensorBoard\n",
    "dummy_input = torch.randn(1, 3, 32, 32).to(device)  # Adjust dimensions as needed\n",
    "writer.add_graph(delta_ltt_model, (dummy_input, torch.tensor(80, device=device), torch.tensor(5, device=device)))\n",
    "\n",
    "# Close the writer\n",
    "writer.close()\n",
    "\n",
    "print(\"Model graph has been added to TensorBoard. Run the following command to view it:\")\n",
    "print(\"tensorboard --logdir=runs/model_visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

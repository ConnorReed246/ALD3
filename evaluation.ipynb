{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../') #act as if we are one directory higher so imports work \n",
    "import torch\n",
    "from latent_to_timestep_model import LTT_model\n",
    "from dataset import load_data_from_dir\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0\"\n",
    "steps = 5\n",
    "latents, targets, conditions, unconditions, optimal_params = load_data_from_dir(data_folder=path, limit=50, use_optimal_params=True, steps=steps)\n",
    "#optimal_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LD3 Best timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n3_params = torch.tensor([0.6048, 1.0274, 0.6334, 1.8439], device='cuda:0')\n",
    "n5_params = torch.tensor([0.8088, 1.1801, 0.9390, 0.7322, 0.7591, 2.0050], device='cuda:0')\n",
    "n7_params = torch.tensor([1.1434, 1.2401, 0.9985, 0.6071, 0.9339, 0.1873, 0.8551, 1.9311], device='cuda:0')\n",
    "n10_params = torch.tensor([1.6245, 1.3128, 1.5374, 0.6975, 0.8498, 0.9843, 1.3483, 0.6511, 1.1129, 1.2806, 1.6264], device='cuda:0')\n",
    "\n",
    "\n",
    "n3_params = F.softmax(n3_params, dim=0)\n",
    "n5_params = F.softmax(n5_params, dim=0)\n",
    "n7_params = F.softmax(n7_params, dim=0)\n",
    "n10_params = F.softmax(n10_params, dim=0)\n",
    "\n",
    "print(f\"n3_params:\\n{n3_params}\")\n",
    "print(f\"n5_params:\\n{n5_params}\")\n",
    "print(f\"n7_params:\\n{n7_params}\")\n",
    "print(f\"n10_params:\\n{n10_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LD3 Timesteps Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "import os\n",
    "\n",
    "from dataset import load_data_from_dir\n",
    "from trainer import LD3Trainer, ModelConfig, TrainingConfig, DiscretizeModelWrapper\n",
    "from utils import (\n",
    "    get_solvers,\n",
    "    parse_arguments,\n",
    "    adjust_hyper,\n",
    "    set_seed_everything,\n",
    "    move_tensor_to_device\n",
    ")\n",
    "from models import prepare_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_params(params: torch.tensor) -> float:    \n",
    "    start_time = time.time()\n",
    "    args = parse_arguments([\n",
    "        \"--all_config\", \"configs/cifar10.yml\",\n",
    "        \"--data_dir\", \"train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0\",\n",
    "        \"--num_train\", \"0\",\n",
    "        \"--num_valid\", \"50\",\n",
    "        \"--steps\", str(len(params)-1),\n",
    "        \"--training_rounds_v1\", \"1\",\n",
    "        \"--seed\", \"0\",\n",
    "    ])\n",
    "\n",
    "    set_seed_everything(args.seed)\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    wrapped_model, _, decoding_fn, noise_schedule, latent_resolution, latent_channel, _, _ = prepare_stuff(args)\n",
    "    adjust_hyper(args, latent_resolution, latent_channel)\n",
    "    solver, steps, solver_extra_params = get_solvers(\n",
    "        args.solver_name,\n",
    "        NFEs=args.steps,\n",
    "        order=args.order,\n",
    "        noise_schedule=noise_schedule,\n",
    "        unipc_variant=args.unipc_variant,\n",
    "    )\n",
    "    latents, targets, _, _, _ = load_data_from_dir( #this is what we take from trainig, targets are original images and latens latent goal\n",
    "        data_folder=args.data_dir, limit=args.num_train + args.num_valid, use_optimal_params=False\n",
    "    )\n",
    "\n",
    "    training_config = TrainingConfig(\n",
    "        train_data=latents,\n",
    "        valid_data=latents,\n",
    "        train_batch_size=args.main_train_batch_size,\n",
    "        valid_batch_size=args.main_valid_batch_size,\n",
    "        lr_time_1=args.lr_time_1,\n",
    "        shift_lr=args.shift_lr,\n",
    "        shift_lr_decay=args.shift_lr_decay,\n",
    "        min_lr_time_1=args.min_lr_time_1,\n",
    "        win_rate=args.win_rate,\n",
    "        patient=args.patient,\n",
    "        lr_time_decay=args.lr_time_decay,\n",
    "        momentum_time_1=args.momentum_time_1,\n",
    "        weight_decay_time_1=args.weight_decay_time_1,\n",
    "        loss_type=args.loss_type,\n",
    "        visualize=args.visualize,\n",
    "        no_v1=args.no_v1,\n",
    "        prior_timesteps=args.gits_ts,\n",
    "        match_prior=args.match_prior,\n",
    "    )\n",
    "    model_config = ModelConfig(\n",
    "        net=wrapped_model,\n",
    "        decoding_fn=decoding_fn,\n",
    "        noise_schedule=noise_schedule,\n",
    "        solver=solver,\n",
    "        solver_name=args.solver_name,\n",
    "        order=args.order,\n",
    "        steps=steps,\n",
    "        prior_bound=args.prior_bound,\n",
    "        resolution=latent_resolution,\n",
    "        channels=latent_channel,\n",
    "        time_mode=args.time_mode,\n",
    "        solver_extra_params=solver_extra_params,\n",
    "        device=device,\n",
    "    )\n",
    "    trainer = LD3Trainer(model_config, training_config)\n",
    "    dis_model = DiscretizeModelWrapper( #Changed through LTT\n",
    "            lambda_max=trainer.lambda_max,\n",
    "            lambda_min=trainer.lambda_min,\n",
    "            noise_schedule=trainer.noise_schedule,\n",
    "            time_mode = trainer.time_mode,\n",
    "        )\n",
    "    loss_list = torch.zeros(len(targets))\n",
    "    for i, (img, latent) in enumerate(zip(targets, latents)):\n",
    "    \n",
    "        img, latent = move_tensor_to_device(img, latent, device = device)\n",
    "        \n",
    "        timestep = dis_model.convert(params.unsqueeze(0))\n",
    "\n",
    "        x_next = trainer.noise_schedule.prior_transformation(latent)\n",
    "        x_next = trainer.solver.sample_simple(\n",
    "            model_fn=trainer.net,\n",
    "            x=x_next,\n",
    "            timesteps=timestep[0],\n",
    "            order=trainer.order,\n",
    "            NFEs=trainer.steps,\n",
    "            **trainer.solver_extra_params,\n",
    "            )\n",
    "        x_next = trainer.decoding_fn(x_next)\n",
    "        trainer.loss_vector = trainer.loss_fn(img.float(), x_next.float()).squeeze()\n",
    "        loss = trainer.loss_vector.mean() \n",
    "        loss_list[i] = loss\n",
    "    print(\"Time taken: \", time.time() - start_time)\n",
    "    return loss_list.mean().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for name, params in zip([\"n3\", \"n5\", \"n7\", \"n10\"], [n3_params, n5_params, n7_params, n10_params]):\n",
    "    print(f\"Loss for {name}: {evaluate_params(params)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/N10-val50-train450-rv12-seed0/final_ltt_model.pt\", 10\n",
    "model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/N10-val50-train50-rv12-seed0/ltt_model.pt\", 10\n",
    "\n",
    "model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/N10-val50-train450-rv12-seed0-fixed_scaling/final_ltt_model.pt\", 10\n",
    "model_path, steps  = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/N7-val50-train450-rv12-seed0-fixed_scaling/final_ltt_model.pt\", 7\n",
    "model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/N5-val50-train450-rv12-seed0-fixed_scaling/final_ltt_model.pt\", 5\n",
    "# model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/N3-val50-train450-rv12-seed0-fixed_scaling/final_ltt_model.pt\", 3\n",
    "model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/LTT_batch3_moreData_N5-val50-train450-r5/final_ltt_model.pt\", 5\n",
    "without_dropout_model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/LTT_after_ltt_change_batch1_without_dropout_N5-val50-train450-r10/final_ltt_model.pt\", 5\n",
    "with_dropout_model_path, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/logs/logs_cifar10/LTT_after_ltt_change_batch1_with_dropout_N5-val50-train450-r10/final_ltt_model.pt\", 5\n",
    "trained_on_optimal_without_dropout, steps = \"/netpool/homes/connor/DiffusionModels/LD3_connor/runs/RandomModels/model_lr0.0001_batch5_without_dropout.pth\", 5\n",
    "\n",
    "\n",
    "ltt_model = LTT_model(steps=steps)\n",
    "state_dict = torch.load(trained_on_optimal_without_dropout, weights_only=True)\n",
    "ltt_model.load_state_dict(state_dict)  # Load the model state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(ltt_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_list = ltt_model.forward(torch.stack(latents)) \n",
    "\n",
    "#visualize as violin plot over each of the 10 timesteps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "params_list = params_list.detach().numpy()\n",
    "params_list = params_list.reshape(-1, steps+1)\n",
    "params_list = pd.DataFrame(params_list, columns=[f\"{i}\" for i in range(steps+1)])\n",
    "sns.violinplot(data=params_list)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-net encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the output\n",
    "hook_storage = {}\n",
    "\n",
    "# Define the hook function using a closure\n",
    "def get_hook(storage):\n",
    "    def hook_fn(module, input, output):\n",
    "        storage[\"unet_output\"] = output  # Store output in the dictionary\n",
    "    return hook_fn\n",
    "# Register the hook on the UNet\n",
    "hook_handle = ltt_model.unet.register_forward_hook(get_hook(hook_storage))\n",
    "\n",
    "# Run the forward pass\n",
    "output = ltt_model.forward(latents[0].unsqueeze(0))\n",
    "\n",
    "# Retrieve the stored UNet output\n",
    "unet_output = hook_storage[\"unet_output\"]\n",
    "print(\"Stored UNet Output:\", unet_output.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(tensor1, tensor2):\n",
    "    return torch.mean((tensor1 - tensor2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = []\n",
    "num_matrices = 20\n",
    "for latent in latents[:num_matrices]:\n",
    "    output = ltt_model.forward(latent.unsqueeze(0))\n",
    "    encodings.append(hook_storage[\"unet_output\"])\n",
    "\n",
    "\n",
    "mse_matrix = np.zeros((num_matrices, num_matrices))\n",
    "\n",
    "for i in range(num_matrices):\n",
    "    for j in range(num_matrices):\n",
    "        mse_matrix[i, j] = mse(encodings[i], encodings[j])\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(mse_matrix, annot=False, cmap=\"viridis\", linewidths=0.5)\n",
    "plt.title(\"Pairwise MSE Heatmap\")\n",
    "plt.xlabel(\"Matrix Index\")\n",
    "plt.ylabel(\"Matrix Index\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Timesteps Per Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from torch.save(loss_matrix, os.path.join(args.data_dir, f\"loss_matrix.pt\"))\n",
    "data_dir = \"/netpool/homes/connor/DiffusionModels/LD3_connor/train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0/\"\n",
    "loss_matrix = torch.load(os.path.join(data_dir, f\"loss_matrix.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(loss_matrix, annot=False, cmap=\"viridis\", linewidths=0.5)\n",
    "plt.title(\"Pairwise MSE Heatmap\")\n",
    "plt.xlabel(\"Matrix Index\")\n",
    "plt.ylabel(\"Matrix Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find lowest loss matrix\n",
    "min_loss = torch.min(loss_matrix, axis=1)\n",
    "min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(2):\n",
    "    min_values = [torch.min(loss_matrix[i, :j+1,]).item() for j in range(300)]\n",
    "    plt.plot(range(300), min_values, label=f'Line {i+1}')\n",
    "plt.title(\"Lowest Value of Second Dimension Up to That Point\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Lowest Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_matrix = torch.load(os.path.join(data_dir, f\"loss_grad_matrix.pt\"))\n",
    "gradient_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(gradient_matrix[2], annot=False, cmap=\"viridis\", linewidths=0.5)\n",
    "plt.title(\"Pairwise MSE Heatmap\")\n",
    "plt.xlabel(\"Matrix Index\")\n",
    "plt.ylabel(\"Matrix Index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(2):\n",
    "    plt.plot(range(300), abs(gradient_matrix[1, :, i]), label=f'Line {i+1}')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Gradient Matrix Lines\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Gradient Value (log scale)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.tensor([1,2,3,4,5,6], dtype=torch.float32)\n",
    "test_tensor2 = torch.tensor([1,2,3,4,5,6], dtype=torch.float32)*2\n",
    "test_tensor3 = torch.tensor([1,2,3,4,5,6], dtype=torch.float32)+1\n",
    "\n",
    "m = nn.Softmax()\n",
    "print(m(test_tensor))\n",
    "print(m(test_tensor2))\n",
    "print(m(test_tensor3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare optimal params for different images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_files = [f for f in os.listdir(path) if \"optimal_params\" in f]\n",
    "for i, file_name in enumerate(sorted(pt_files)[:]): #load all training files previously created\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    params, loss = torch.load(file_path, weights_only=True)\n",
    "    data = params.detach().numpy()\n",
    "    losses = loss.detach().numpy()\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    # Apply PCA to reduce to 2 dimensions\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(data_scaled)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(principal_components[:, 0], principal_components[:, 1],\n",
    "                        c=losses, cmap='viridis', edgecolor='k', s=100)\n",
    "    plt.colorbar(scatter, label='Loss')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('PCA Visualization Colored by Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(\"params\\n\", data)\n",
    "    print(\"losses\\n\", losses)\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_files = [f for f in os.listdir(path) if \"optimal_params\" in f]\n",
    "pt_files = sorted(pt_files)[:6]  # Load first 6 training files\n",
    "\n",
    "shapes = ['circle', 'square', 'diamond', 'triangle-up', 'triangle-down', 'cross']  # Different marker shapes\n",
    "\n",
    "all_data = []\n",
    "best_losses = []\n",
    "\n",
    "\n",
    "\n",
    "for i, file_name in enumerate(pt_files):\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    # Load parameters and loss\n",
    "    params, loss = torch.load(file_path, weights_only=True)  \n",
    "    data = params.detach().numpy()\n",
    "    losses = loss.detach().numpy()\n",
    "    all_data.append(data)\n",
    "    best_losses.append(losses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_data = np.stack(all_data).reshape(-1, steps+1)\n",
    "best_losses = np.stack(best_losses).reshape(-1)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(all_data)\n",
    "    \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(principal_components, columns=[\"PC1\", \"PC2\"])\n",
    "df[\"Loss\"] = best_losses\n",
    "df[\"Shape\"] = [x for x in shapes for _ in range(10)]\n",
    "df[\"params\"] = list(all_data)\n",
    "df[\"params\"] = df[\"params\"].apply(lambda x: [round(v, 4) for v in x])\n",
    "\n",
    "# # Create interactive 3D scatter plot using Plotly\n",
    "fig = px.scatter(df, x=\"PC1\", y=\"PC2\", color=\"Loss\", symbol=\"Shape\",\n",
    "                    hover_data={\"Loss\": \":.4f\", \"PC1\": False, \"PC2\": False, \"params\": True},\n",
    "                    color_continuous_scale=\"viridis\",\n",
    "                    title=\"2D PCA Visualization Colored by Loss with Different Shapes\")\n",
    "\n",
    "# Move color bar to the left\n",
    "fig.update_layout(coloraxis_colorbar=dict(x=-0.2))  \n",
    "\n",
    "fig.update_traces(marker=dict(size=5, line=dict(width=1, color=\"black\")))\n",
    "\n",
    "# Show interactive 3D plot\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_files = [f for f in os.listdir(path) if \"optimal_params\" in f]\n",
    "pt_files = sorted(pt_files)[:6]  # Load first 6 training files\n",
    "\n",
    "shapes = ['circle', 'square', 'diamond', 'triangle-up', 'triangle-down', 'cross']  # Different marker shapes\n",
    "\n",
    "all_data = []\n",
    "best_losses = []\n",
    "\n",
    "\n",
    "\n",
    "for i, file_name in enumerate(pt_files):\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    # Load parameters and loss\n",
    "    params, loss = torch.load(file_path, weights_only=True)  \n",
    "    data = params.detach().numpy()\n",
    "    losses = loss.detach().numpy()\n",
    "    all_data.append(data)\n",
    "    best_losses.append(losses)\n",
    "\n",
    "\n",
    "all_data = np.stack(all_data).reshape(-1, steps+1)\n",
    "best_losses = np.stack(best_losses).reshape(-1)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "principal_components = pca.fit_transform(all_data)\n",
    "    \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(principal_components, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "df[\"Loss\"] = best_losses\n",
    "df[\"Shape\"] = [x for x in shapes for _ in range(10)]\n",
    "df[\"params\"] = list(all_data)\n",
    "df[\"params\"] = df[\"params\"].apply(lambda x: [round(v, 4) for v in x])\n",
    "\n",
    "# # Create interactive 3D scatter plot using Plotly\n",
    "fig = px.scatter_3d(df, x=\"PC1\", y=\"PC2\", z=\"PC3\", color=\"Loss\", symbol=\"Shape\",\n",
    "                    hover_data={\"Loss\": \":.4f\", \"PC1\": False, \"PC2\": False, \"PC3\": False, \"params\": True},\n",
    "                    color_continuous_scale=\"viridis\",\n",
    "                    title=\"3D PCA Visualization Colored by Loss with Different Shapes\")\n",
    "\n",
    "# Move color bar to the left\n",
    "fig.update_layout(coloraxis_colorbar=dict(x=-0.2))  \n",
    "\n",
    "fig.update_traces(marker=dict(size=5, line=dict(width=1, color=\"black\")))\n",
    "\n",
    "# Show interactive 3D plot\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.histplot(best_losses, bins=50, kde=True)\n",
    "plt.title(\"Loss Distribution\")\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only best trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_files = [f for f in os.listdir(path) if \"optimal_params\" in f]\n",
    "pt_files = sorted(pt_files) # Load first 6 training files\n",
    "\n",
    "all_data = []\n",
    "best_losses = []\n",
    "all_losses = []\n",
    "\n",
    "for i, file_name in enumerate(pt_files):\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    # Load parameters and loss\n",
    "    params, loss = torch.load(file_path, weights_only=True)  \n",
    "    data = params.detach().numpy()[0]\n",
    "    losses = loss.detach().numpy()\n",
    "    all_data.append(data)\n",
    "    best_losses.append(losses[0])\n",
    "    all_losses.append(losses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_data = np.stack(all_data).reshape(-1, steps+1)\n",
    "best_losses = np.stack(best_losses).reshape(-1)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "principal_components = pca.fit_transform(all_data)\n",
    "    \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(principal_components, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "df[\"Loss\"] = best_losses\n",
    "df[\"params\"] = list(all_data)\n",
    "df[\"params\"] = df[\"params\"].apply(lambda x: [round(v, 4) for v in x])\n",
    "\n",
    "# # Create interactive 3D scatter plot using Plotly\n",
    "fig = px.scatter_3d(df, x=\"PC1\", y=\"PC2\", z=\"PC3\", color=\"Loss\",\n",
    "                    hover_data={\"Loss\": \":.4f\", \"PC1\": False, \"PC2\": False, \"PC3\": False, \"params\": True},\n",
    "                    color_continuous_scale=\"viridis\",\n",
    "                    title=\"3D PCA Visualization Colored by Loss with Different Shapes\")\n",
    "\n",
    "# Move color bar to the left\n",
    "fig.update_layout(coloraxis_colorbar=dict(x=-0.2))  \n",
    "\n",
    "fig.update_traces(marker=dict(size=5, line=dict(width=1, color=\"black\")))\n",
    "\n",
    "# Show interactive 3D plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss distribution of all_losses\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.histplot(best_losses, bins=50, kde=True)\n",
    "plt.title(\"Loss Distribution\")\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming all_losses and best_losses are defined\n",
    "all_losses = np.stack(all_losses).reshape(-1)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add histogram for all_losses\n",
    "fig.add_trace(go.Histogram(x=all_losses, nbinsx=75, histnorm='probability density', \n",
    "                           name=\"All Losses\", marker_color='blue', opacity=0.5))\n",
    "\n",
    "# Add histogram for best_losses\n",
    "fig.add_trace(go.Histogram(x=best_losses, nbinsx=75, histnorm='probability density', \n",
    "                           name=\"Best Losses\", marker_color='red', opacity=0.5))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Loss Distribution\",\n",
    "    xaxis_title=\"Loss\",\n",
    "    yaxis_title=\"Density\",\n",
    "    barmode='overlay',  # Overlay both histograms\n",
    "    template=\"plotly_white\"  # Optional: use a clean background\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print(f\"Mean lost of first 50 validation losses: {np.mean(all_losses[:500]):.4f}\")\n",
    "print(f\"Mean lost of 50 validation best losses: {np.mean(best_losses[:50]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

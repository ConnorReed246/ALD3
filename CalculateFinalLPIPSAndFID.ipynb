{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1745b86a",
   "metadata": {},
   "source": [
    "# Calculating Final Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e50f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import lpips\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "\n",
    "from dataset import LTTDataset, load_data_from_dir\n",
    "from latent_to_timestep_model import Delta_LTT_model, LTT_model\n",
    "from models import prepare_stuff\n",
    "from trainer import LD3Trainer, ModelConfig, TrainingConfig, DiscretizeModelWrapper\n",
    "from utils import (\n",
    "    adjust_hyper,\n",
    "    get_solvers,\n",
    "    move_tensor_to_device,\n",
    "    parse_arguments,\n",
    "    set_seed_everything,\n",
    "    visual\n",
    ")\n",
    "\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "sys.path.append('../')  # Act as if we are one directory higher so imports work\n",
    "img_save_dir = 'graphics/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f6ccb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/netpool/homes/connor/anaconda3/envs/ld3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/netpool/homes/connor/anaconda3/envs/ld3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /netpool/homes/connor/anaconda3/envs/ld3/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/netpool/homes/connor/anaconda3/envs/ld3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/netpool/homes/connor/anaconda3/envs/ld3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /netpool/homes/connor/anaconda3/envs/ld3/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "args = parse_arguments([\n",
    "    \"--all_config\", \"configs/cifar10.yml\",\n",
    "    \"--data_dir\", \"train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0\",\n",
    "    \"--num_train\", \"1000\",\n",
    "    \"--num_valid\", \"1000\",\n",
    "    \"--main_train_batch_size\", \"200\",\n",
    "    \"--main_valid_batch_size\", \"200\",\n",
    "    \"--training_rounds_v1\", \"1\",\n",
    "    \"--log_path\", \"logs/logs_cifar10\",\n",
    "    \"--force_train\", \"True\",\n",
    "    \"--steps\", \"5\",\n",
    "    \"--lr_time_1\", \"0.00005\",\n",
    "    \"--mlp_dropout\", \"0.0\",\n",
    "    \"--log_suffix\", \"BiggerValidation_GroupNorm_EvalTrue\"\n",
    "])\n",
    "\n",
    "set_seed_everything(args.seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Dataset\n",
    "data_dir = 'train_data/train_data_cifar10/uni_pc_NFE20_edm_seed0'\n",
    "model_dir = \"runs_delta_timesteps/models\"\n",
    "steps = 5\n",
    "lpips_loss_fn = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "\n",
    "wrapped_model, _, decoding_fn, noise_schedule, latent_resolution, latent_channel, _, _ = prepare_stuff(args)\n",
    "solver, steps, solver_extra_params = get_solvers(\n",
    "    args.solver_name,\n",
    "    NFEs=args.steps,\n",
    "    order=args.order,\n",
    "    noise_schedule=noise_schedule,\n",
    "    unipc_variant=args.unipc_variant,\n",
    ")\n",
    "\n",
    "order = args.order  \n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    collated_batch = []\n",
    "    for samples in zip(*batch):\n",
    "        if any(item is None for item in samples):\n",
    "            collated_batch.append(None)\n",
    "        else:\n",
    "            collated_batch.append(torch.utils.data._utils.collate.default_collate(samples))\n",
    "    return collated_batch\n",
    "\n",
    "valid_dataset = LTTDataset(dir=os.path.join(data_dir, \"validation\"), size=args.num_valid, train_flag=False, use_optimal_params=False) \n",
    "train_dataset = LTTDataset(dir=os.path.join(data_dir, \"train\"), size=args.num_train, train_flag=True, use_optimal_params=False)\n",
    "\n",
    "delta_ltt_model = Delta_LTT_model(steps = steps, mlp_dropout=args.mlp_dropout)\n",
    "delta_ltt_model = delta_ltt_model.to(device)\n",
    "\n",
    "wrapped_model, _, decoding_fn, noise_schedule, latent_resolution, latent_channel, _, _ = prepare_stuff(args)\n",
    "solver, steps, solver_extra_params = get_solvers(\n",
    "    args.solver_name,\n",
    "    NFEs=args.steps,\n",
    "    order=args.order,\n",
    "    noise_schedule=noise_schedule,\n",
    "    unipc_variant=args.unipc_variant,\n",
    ")\n",
    "training_config = TrainingConfig(\n",
    "    train_data=train_dataset,\n",
    "    valid_data=valid_dataset,\n",
    "    train_batch_size=args.main_train_batch_size,\n",
    "    valid_batch_size=args.main_valid_batch_size,\n",
    "    lr_time_1=args.lr_time_1,\n",
    "    shift_lr=args.shift_lr,\n",
    "    shift_lr_decay=args.shift_lr_decay,\n",
    "    min_lr_time_1=args.min_lr_time_1,\n",
    "    win_rate=args.win_rate,\n",
    "    patient=args.patient,\n",
    "    lr_time_decay=args.lr_time_decay,\n",
    "    momentum_time_1=args.momentum_time_1,\n",
    "    weight_decay_time_1=args.weight_decay_time_1,\n",
    "    loss_type=args.loss_type,\n",
    "    visualize=args.visualize,\n",
    "    no_v1=args.no_v1,\n",
    "    prior_timesteps=args.gits_ts,\n",
    "    match_prior=args.match_prior,\n",
    ")\n",
    "model_config = ModelConfig(\n",
    "    net=wrapped_model,\n",
    "    decoding_fn=decoding_fn,\n",
    "    noise_schedule=noise_schedule,\n",
    "    solver=solver,\n",
    "    solver_name=args.solver_name,\n",
    "    order=args.order,\n",
    "    steps=steps,\n",
    "    prior_bound=args.prior_bound,\n",
    "    resolution=latent_resolution,\n",
    "    channels=latent_channel,\n",
    "    time_mode=args.time_mode,\n",
    "    solver_extra_params=solver_extra_params,\n",
    "    device=device,\n",
    ")\n",
    "trainer = LD3Trainer(model_config, training_config)\n",
    "\n",
    "\n",
    "dis_model = DiscretizeModelWrapper( #Changed through LTT\n",
    "        lambda_max=trainer.lambda_max,\n",
    "        lambda_min=trainer.lambda_min,\n",
    "        noise_schedule=trainer.noise_schedule,\n",
    "        time_mode = trainer.time_mode,\n",
    "    )\n",
    "\n",
    "\n",
    "img, latent, _ = valid_dataset[0]\n",
    "latent = latent.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767d5e0",
   "metadata": {},
   "source": [
    "## Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4ccc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#My implementation\n",
    "\n",
    "n3_params = torch.tensor([0.3125, 0.1682, 0.1343, 0.3851], device='cuda:0')\n",
    "n3_timestep = torch.tensor([[8.0000e+01, 5.9884e+00, 7.5587e-01, 2.0000e-03]], device='cuda:0')\n",
    "\n",
    "n5_params = torch.tensor([0.2225, 0.1482, 0.1034, 0.0818, 0.0839, 0.3603], device='cuda:0')\n",
    "n5_timestep = torch.tensor([[8.0000e+01, 1.0621e+01, 2.5949e+00, 8.5124e-01, 2.7130e-01, 2.0000e-03]],\n",
    "       device='cuda:0')\n",
    "\n",
    "n6_params = torch.tensor([0.0437, 0.1651, 0.1108, 0.0790, 0.1370, 0.0390, 0.1000, 0.3254],\n",
    "       device='cuda:0')\n",
    "n6_timestep = torch.tensor([[8.0000e+01, 1.2833e+01, 3.7612e+00, 1.5670e+00, 3.4353e-01, 2.2291e-01,\n",
    "         7.3629e-02, 2.0000e-03]], device='cuda:0')\n",
    "\n",
    "n7_params = torch.tensor([0.1246, 0.1541, 0.1057, 0.0844, 0.0897, 0.0760, 0.3655],\n",
    "       device='cuda:0')\n",
    "n7_timestep = torch.tensor([[8.0000e+01, 1.2389e+01, 3.4480e+00, 1.2409e+00, 4.1899e-01, 1.6694e-01,\n",
    "         2.0000e-03]], device='cuda:0')\n",
    "\n",
    "n10_params = torch.tensor([0.0994, 0.1309, 0.0971, 0.0447, 0.0592, 0.0616, 0.1001, 0.0391, 0.0728,\n",
    "        0.0754, 0.2196], device='cuda:0')\n",
    "n10_timestep =  torch.tensor([[8.0000e+01, 1.7152e+01, 5.4689e+00, 3.2309e+00, 1.6107e+00, 7.8026e-01,\n",
    "         2.4016e-01, 1.5164e-01, 6.4386e-02, 2.6514e-02, 2.0000e-03]],\n",
    "       device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68bea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1018408/3391317573.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ld3_n6_dict = torch.load(ld3_n6_path, map_location=device)\n",
      "/tmp/ipykernel_1018408/3391317573.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ld3_n3_dict = torch.load(ld3_n3_path, map_location=device)\n",
      "/tmp/ipykernel_1018408/3391317573.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ld3_n5_dict = torch.load(ld3_n5_path, map_location=device)\n",
      "/tmp/ipykernel_1018408/3391317573.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ld3_n7_dict = torch.load(ld3_n7_path, map_location=device)\n",
      "/tmp/ipykernel_1018408/3391317573.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ld3_n10_dict = torch.load(ld3_n10_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "#LD3\n",
    "# Load timesteps for n6\n",
    "ld3_n6_path = \"/netpool/homes/connor/LD3_main/logs/logs_cifar10/LD3_correctedLatents_N6-val200-train10000-rv11-seed0/best_v2.pt\"\n",
    "ld3_n6_dict = torch.load(ld3_n6_path, map_location=device)\n",
    "ld3_n6_timestep = ld3_n6_dict['best_t_steps'][:len(ld3_n6_dict['best_t_steps']) // 2]\n",
    "\n",
    "# Load timesteps for n3\n",
    "ld3_n3_path = \"/netpool/homes/connor/LD3_main/logs/logs_cifar10/LD3_correctedLatents_N3-val200-train10000-rv11-seed0/best_v2.pt\"\n",
    "ld3_n3_dict = torch.load(ld3_n3_path, map_location=device)\n",
    "ld3_n3_timestep = ld3_n3_dict['best_t_steps'][:len(ld3_n3_dict['best_t_steps']) // 2]\n",
    "\n",
    "# Load timesteps for n5\n",
    "ld3_n5_path = \"/netpool/homes/connor/LD3_main/logs/logs_cifar10/LD3_correctedLatents_N5-val200-train10000-rv11-seed0/best_v2.pt\"\n",
    "ld3_n5_dict = torch.load(ld3_n5_path, map_location=device)\n",
    "ld3_n5_timestep = ld3_n5_dict['best_t_steps'][:len(ld3_n5_dict['best_t_steps']) // 2]\n",
    "\n",
    "# Load timesteps for n7\n",
    "ld3_n7_path = \"/netpool/homes/connor/LD3_main/logs/logs_cifar10/LD3_correctedLatents_N7-val200-train10000-rv11-seed0/best_v2.pt\"\n",
    "ld3_n7_dict = torch.load(ld3_n7_path, map_location=device)\n",
    "ld3_n7_timestep = ld3_n7_dict['best_t_steps'][:len(ld3_n7_dict['best_t_steps']) // 2]\n",
    "\n",
    "# Load timesteps for n10\n",
    "ld3_n10_path = \"/netpool/homes/connor/LD3_main/logs/logs_cifar10/LD3_correctedLatents_N10-val200-train10000-rv11-seed0/best_v2.pt\"\n",
    "ld3_n10_dict = torch.load(ld3_n10_path, map_location=device)\n",
    "ld3_n10_timestep = ld3_n10_dict['best_t_steps'][:len(ld3_n10_dict['best_t_steps']) // 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6a894",
   "metadata": {},
   "source": [
    "### LPIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733cd6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_params(params: torch.tensor) -> float:    \n",
    "    start_time = time.time()\n",
    "    loss_list = []\n",
    "    for i,batch in enumerate(trainer.valid_only_loader):\n",
    "        print(i)\n",
    "        if i*len(batch[0]) < 200: #skip the first 200 to just test on unseen data\n",
    "            continue\n",
    "        img, latent, _ = batch\n",
    "        latent = latent.to(device)\n",
    "        img = img.to(device)\n",
    "        \n",
    "        timestep = dis_model.convert(params.unsqueeze(0))\n",
    "        print(timestep)\n",
    "\n",
    "        x_next = trainer.noise_schedule.prior_transformation(latent)\n",
    "        x_next = trainer.solver.sample_simple(\n",
    "            model_fn=trainer.net,\n",
    "            x=x_next,\n",
    "            timesteps=timestep[0],\n",
    "            order=trainer.order,\n",
    "            NFEs=trainer.steps,\n",
    "            **trainer.solver_extra_params,\n",
    "            )\n",
    "        x_next = trainer.decoding_fn(x_next)\n",
    "        trainer.loss_vector = trainer.loss_fn(img.float(), x_next.float()).squeeze()\n",
    "        loss = trainer.loss_vector.mean().item() \n",
    "        loss_list.append(loss)\n",
    "    print(\"Time taken: \", time.time() - start_time)\n",
    "    return np.mean(loss_list)\n",
    "\n",
    "\n",
    "for name, params in zip([\"n3\", \"n5\", \"n6\", \"n7\", \"n10\"], [n3_params, n5_params,n6_params, n7_params, n10_params]):\n",
    "    print(f\"Loss for {name}: {evaluate_params(params)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ee18f",
   "metadata": {},
   "source": [
    "Time taken:  21.534754753112793  \n",
    "Loss for n3: 0.27483632415533066  \n",
    "Time taken:  35.639776945114136  \n",
    "Loss for n5: 0.14499929174780846  \n",
    "Time taken:  49.86462879180908  \n",
    "Loss for n6: 0.08154943212866783  \n",
    "Time taken:  42.588207721710205  \n",
    "Loss for n7: 0.11239975690841675  \n",
    "Time taken:  70.93965196609497  \n",
    "Loss for n10: 0.027583551593124866  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c8bb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  8.304322481155396\n",
      "Loss for n3: 0.2787010371685028\n",
      "Time taken:  12.00994610786438\n",
      "Loss for n5: 0.1452961415052414\n",
      "Time taken:  14.24762487411499\n",
      "Loss for n6: 0.11326901987195015\n",
      "Time taken:  16.460753917694092\n",
      "Loss for n7: 0.08341463096439838\n",
      "Time taken:  23.243257999420166\n",
      "Loss for n10: 0.03247475391253829\n"
     ]
    }
   ],
   "source": [
    "def evaluate_timestep(timestep: torch.tensor) -> float:    \n",
    "    start_time = time.time()\n",
    "    loss_list = []\n",
    "    for i,batch in enumerate(trainer.valid_only_loader):\n",
    "        if i*len(batch[0]) < 200: #skip the first 200 to just test on unseen data\n",
    "            continue\n",
    "        img, latent, _ = batch\n",
    "        latent = latent.to(device)\n",
    "        img = img.to(device)\n",
    "        \n",
    "        x_next = trainer.noise_schedule.prior_transformation(latent)\n",
    "        x_next = trainer.solver.sample_simple(\n",
    "            model_fn=trainer.net,\n",
    "            x=x_next,\n",
    "            timesteps=timestep,\n",
    "            order=trainer.order,\n",
    "            NFEs=trainer.steps,\n",
    "            **trainer.solver_extra_params,\n",
    "            )\n",
    "        x_next = trainer.decoding_fn(x_next)\n",
    "        trainer.loss_vector = trainer.loss_fn(img.float(), x_next.float()).squeeze()\n",
    "        loss = trainer.loss_vector.mean().item() \n",
    "        loss_list.append(loss)\n",
    "    print(\"Time taken: \", time.time() - start_time)\n",
    "    return np.mean(loss_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for name, timestep in zip([\"n3\", \"n5\", \"n6\", \"n7\", \"n10\"], [ld3_n3_timestep, ld3_n5_timestep, ld3_n6_timestep, ld3_n7_timestep, ld3_n10_timestep]):\n",
    "    print(f\"Loss for {name}: {evaluate_timestep(timestep)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1e8a07",
   "metadata": {},
   "source": [
    "Time taken:  7.843273401260376  \n",
    "Loss for n3: 0.2787010371685028  \n",
    "Time taken:  12.100373268127441  \n",
    "Loss for n5: 0.1452961415052414  \n",
    "Time taken:  14.393749952316284   \n",
    "Loss for n6: 0.11326901987195015  \n",
    "Time taken:  16.84900426864624  \n",
    "Loss for n7: 0.08341463096439838  \n",
    "Time taken:  23.266944408416748  \n",
    "Loss for n10: 0.03293218836188316  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a709c",
   "metadata": {},
   "source": [
    "### FID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1ee7a",
   "metadata": {},
   "source": [
    "https://github.com/toshas/torch-fidelity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f72621f",
   "metadata": {},
   "source": [
    "##### Reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d611e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Images for n3: 100%|██████████| 100/100 [06:42<00:00,  4.02s/it]\n",
      "Generating Images for n5:  60%|██████    | 60/100 [06:41<04:28,  6.71s/it]"
     ]
    }
   ],
   "source": [
    "batch_size = 500\n",
    "number_of_fid_images = 50000\n",
    "shape = (batch_size, 3, 32, 32)\n",
    "for name, timestep in zip([\"n3\", \"n5\", \"n6\", \"n7\", \"n10\"], [n3_timestep, n5_timestep, n6_timestep, n7_timestep, n10_timestep]):\n",
    "    generator = torch.Generator(torch.device(device))\n",
    "    timestep = timestep[0]\n",
    "    generated_images = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(number_of_fid_images // batch_size), desc=f\"Generating Images for {name}\"):\n",
    "            latent = torch.randn(shape, device=torch.device(device), generator=generator)\n",
    "            x_next = trainer.noise_schedule.prior_transformation(latent)\n",
    "            x_next = trainer.solver.sample_simple(\n",
    "                model_fn=trainer.net,\n",
    "                x=x_next,\n",
    "                timesteps=timestep,\n",
    "                order=trainer.order,\n",
    "                NFEs=trainer.steps,\n",
    "                **trainer.solver_extra_params,\n",
    "            )\n",
    "            x_next = trainer.decoding_fn(x_next)\n",
    "            generated_images.append(x_next)\n",
    "\n",
    "        generated_images = torch.cat(generated_images, dim=0)\n",
    "        save_path = \"/netpool/homes/connor/DiffusionModels/LD3_connor/fid-generated\"\n",
    "        dir_name = f\"global_{name}\"\n",
    "        dir_path = os.path.join(save_path, dir_name)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        for i, img in enumerate(generated_images):\n",
    "            save_image(img, os.path.join(dir_path, f\"{i}.png\"), normalize=True)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52708220",
   "metadata": {},
   "source": [
    "#### LD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e2121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, timestep in zip([\"n3\", \"n5\", \"n6\", \"n7\", \"n10\"], [ld3_n3_timestep, ld3_n5_timestep, ld3_n6_timestep, ld3_n7_timestep, ld3_n10_timestep]):\n",
    "    generator = torch.Generator(torch.device(device))\n",
    "    timestep = timestep[0]\n",
    "    generated_images = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(number_of_fid_images // batch_size), desc=f\"Generating Images for {name}\"):\n",
    "            latent = torch.randn(shape, device=torch.device(device), generator=generator)\n",
    "            x_next = trainer.noise_schedule.prior_transformation(latent)\n",
    "            x_next = trainer.solver.sample_simple(\n",
    "                model_fn=trainer.net,\n",
    "                x=x_next,\n",
    "                timesteps=timestep,\n",
    "                order=trainer.order,\n",
    "                NFEs=trainer.steps,\n",
    "                **trainer.solver_extra_params,\n",
    "            )\n",
    "            x_next = trainer.decoding_fn(x_next)\n",
    "            generated_images.append(x_next)\n",
    "\n",
    "        generated_images = torch.cat(generated_images, dim=0)\n",
    "        save_path = \"/netpool/homes/connor/DiffusionModels/LD3_connor/fid-generated\"\n",
    "        dir_name = f\"global_{name}\"\n",
    "        dir_path = os.path.join(save_path, dir_name)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        for i, img in enumerate(generated_images):\n",
    "            save_image(img, os.path.join(dir_path, f\"{i}.png\"), normalize=True)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f66f63",
   "metadata": {},
   "source": [
    "## Single Shot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3c6227",
   "metadata": {},
   "source": [
    "## Recursive Delta Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e73ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bd009d4",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_ref_path = \"/netpool/homes/connor/DiffusionModels/LD3_connor/fid-refs/cifar10-32x32.npz\"\n",
    "fid_ref_data = np.load(fid_ref_path)\n",
    "mu_real = fid_ref_data['mu']\n",
    "sigma_real = fid_ref_data['sigma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f276c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = FrechetInceptionDistance(feature=2048).to(device)\n",
    "generated_images = generated_images.to(dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ef631",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_batch_size = 100\n",
    "for i in tqdm(range(0, number_of_fid_images, fid_batch_size), desc=\"Calculating FID\"):\n",
    "    batch = generated_images[i:i + fid_batch_size]\n",
    "    fid.update(batch, real=False)\n",
    "torch.cuda.empty_cache()\n",
    "# Compute FID statistics\n",
    "\n",
    "# Calculate FID\n",
    "fid_value = fid.compute_frechet_distance(mu_real, sigma_real, mu_gen, sigma_gen)\n",
    "print(f\"FID: {fid_value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
